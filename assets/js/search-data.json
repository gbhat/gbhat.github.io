{"0": {
    "doc": "Binary Search",
    "weight": 2,
    "title": "Binary Search",
    "description": "Visualization and Implementation of Binary Search",
    "content": "Binary Search is a searching algorithms which works on sorted array. At every iteration it finds the middle element of the array using $mid = \\frac{(low + high)}{2}$ If middle element is equal to the key, search stops. If the middle element is greater than the key, search continues in the first half. Otherwise search continues in the second half. It has best case complexity of O(1), average and worst case complexity of O(log n). Visualization of Binary Search . Implementation of Binary Search . | Java | Python | C | . Copied! . Copied! . Copied! . ",
    "url": "https://gbhat.com/algorithms/binary_search.html",
    "relUrl": "/algorithms/binary_search.html"
  },"1": {
    "doc": "Brute-Force String Search Algorithm",
    "weight": 1,
    "title": "Brute-Force String Search Algorithm",
    "description": "Visualization and Implementation of Brute-Force String Search String Search Algorithm",
    "content": "Brute-Force or Naive String Search algorithm searches for a string (also called pattern) within larger string. It checks for character matches of pattern at each index of string. If all characters of pattern match with string then search stops. If not, it shifts to the next index of string for check. It has worst case complexity of O(mn). Where m is length of pattern and n is length of string. Visualization of Brute-Force String Search Algorithm . Implementation of Brute-Force String Search Algorithm . | Java | Python | C | . Copied! . Copied! . Copied! . ",
    "url": "https://gbhat.com/algorithms/bruteforce_string_search.html",
    "relUrl": "/algorithms/bruteforce_string_search.html"
  },"2": {
    "doc": "Bubble Sort",
    "weight": 1,
    "title": "Bubble Sort",
    "description": "Visualization and Implementation of Bubble Sort",
    "content": "Bubble Sort is a simple comparison based sorting algorithm. It sorts the elements by comparing the adjacent elements and re-arranging them if they are in the wrong order. After every iteration the highest element moves to the end of the array. It has best case complexity of O(n), average and worst case complexity of O($n^2$). Visualization of Bubble Sort . Implementation of Bubble Sort . | Java | Python | C | . Copied! . Copied! . Copied! . ",
    "url": "https://gbhat.com/algorithms/bubble_sort.html",
    "relUrl": "/algorithms/bubble_sort.html"
  },"3": {
    "doc": "Gradient Descent",
    "weight": 4,
    "title": "Gradient Descent",
    "description": "Visualization and Implementation of Gradient Descent",
    "content": "Figure 1: Gradient Descent on a convex function . Gradient Descent is an optimization algorithm. We can use this algorithm to find minimum of a function. This algorithm works iteratively by moving towards a lower point at every step. Once it find no lower points further it stops. Mathematically this algorithm can be written as: $$ x_{n+1} = x_n - \\alpha * \\frac {d f(x)}{dx_n} $$ . Here $\\alpha$ is learning rate. The value of $x_{n+1}$ is computed iteratively until it converges at a minimum. Learning rate is a hyperparameter. Value of learning rate controls the steps size. It is important to have good value for learning rate, since having a very low value may take a long time to converge and having a very high value may not converge at all. We can see the impact of learning rate on the algorithm in Gradient Descent Learning Rates . If the target function is not a convex function and contians local and global minima, gradient descent may converge at local minima instead of global, depending on the starting point. There are variations of the algorithm which can work better especially in non-convex functions. See Gradient Descent with Momentum and Optimization with Momentum . Below program finds the minimum of a convex function using gradient descent from Tensorflow library. Implementation of Gradient Descent . Copied! . ",
    "url": "https://gbhat.com/machine_learning/gradient_descent.html",
    "relUrl": "/machine_learning/gradient_descent.html"
  },"4": {
    "doc": "Gradient Descent Animation",
    "weight": 4,
    "title": "Gradient Descent Animation",
    "description": "Gradient Descent with animation",
    "content": "Figure 1: Gradient Descent on a convex function . In Gradient Descent example we saw how we can minimize a function. If we want to show the animation of gradient descent we can write the program as below. Below program uses matplotlib and FuncAnimation to show the animation. At each iteration of gradient descent, we store the values of (x, y) in a list. In animation function we iterate this list to show the animation. Implementation of Gradient Descent with animation . Copied! . ",
    "url": "https://gbhat.com/machine_learning/gradient_descent_anim.html",
    "relUrl": "/machine_learning/gradient_descent_anim.html"
  },"5": {
    "doc": "Gradient Descent Learning Rates",
    "weight": 4,
    "title": "Gradient Descent Learning Rates",
    "description": "Different Learning Rates in Gradient Descent",
    "content": "Figure 1: Gradient descent with different learning rates . Gradient Descent is defined mathematically as: . $$ x_{n+1} = x_n - \\alpha * \\frac {d f(x)}{dx_n} $$ . Where $\\alpha$ is learning rate. Learning rate is a hyperparameter. Value of learning rate controls the steps size. Having a good learning rate is important because very low value of learning rate may take a long time to converge and having a very high value may not converge at all. In Figure 1 we can see the impact of different learning rates on gradient descent. Below program shows different learning rates for gradient descent using Tensorflow library. Implementation of Gradient Descent Learning Rates . Copied! . ",
    "url": "https://gbhat.com/machine_learning/gradient_descent_learning_rates.html",
    "relUrl": "/machine_learning/gradient_descent_learning_rates.html"
  },"6": {
    "doc": "Gradient Descent Nesterov Momentum",
    "weight": 5,
    "title": "Gradient Descent Nesterov Momentum",
    "description": "Visualization and Implementation of Gradient Descent Nesterov Momentum",
    "content": "Figure 1: Gradient Descent Nesterov Momentum . Nesterov momentum or Nesterov accelerated gradient (NAG) is an improvement over Gradient Descnent with momentum. Mathematically this algorithm can be written as: $$ v_{n+1} = \\beta * v_n + \\frac {d f(x - \\beta * v_n)}{dx_n} $$ $$ x_{n+1} = x_n - \\alpha * v_{n+1} $$ . Here $\\beta$ is momentum term and $\\alpha$ is learning rate. Momentum term $\\beta$ is usually set to 0.9 or nearby value. We can see in above animation that Nesterov momentum converges faster than Gradient Descent with Momentum. For more details on Nesterov Momentum visit: . Stanford CS231n course . https://ruder.io/optimizing-gradient-descent/index.html#nesterovacceleratedgradient . Implementation of Gradient Descent Nesterov Momentum . Copied! . ",
    "url": "https://gbhat.com/machine_learning/gradient_descent_nesterov.html",
    "relUrl": "/machine_learning/gradient_descent_nesterov.html"
  },"7": {
    "doc": "Gradient Descent with Momentum",
    "weight": 4,
    "title": "Gradient Descent with Momentum",
    "description": "Gradient Descent with Momentum",
    "content": "Figure 1: Gradient Descent with momentum on a convex function . Figure 2: Gradient Descent with momentum on a non-convex function . We saw how we can use Gradient Descent to find minimum of a function. Gradient Descent with Momentum is a variation which can be useful in certain situations. This algorithm is analogous to rolling a ball down a slope. Ball slowly accumulates momentum and rolls faster and faster. Momentum helps in crossing some of the local minima if the function is not a convex function. This we can see in Figure 2. Mathematically this algorithm can be written as: $$ v_{n+1} = \\beta * v_n + \\frac {d f(x)}{dx_n} $$ $$ x_{n+1} = x_n - \\alpha * v_{n+1} $$ . Here $\\beta$ is momentum term and $\\alpha$ is learning rate. Momentum term $\\beta$ is usually set to 0.9 or nearby value. If we set $\\beta$ to 0, we arrive at gradient descent formula. Implementation of Gradient Descent with Momentum on convex function . Copied! . Implementation of Gradient Descent with Momentum on non-convex function . Copied! . ",
    "url": "https://gbhat.com/machine_learning/gradient_descent_with_momentum.html",
    "relUrl": "/machine_learning/gradient_descent_with_momentum.html"
  },"8": {
    "doc": "Heap Sort",
    "weight": 2,
    "title": "Heap Sort",
    "description": "Visualization and Implementation of Heap Sort",
    "content": "Heap Sort is a comparison based sorting algorithm. It partitions the array into sorted and unsorted partitions. It builds a Max Heap of unsorted partition. At every iteration, it extracts the maximum element from Heap and move it to the sorted partition. This operation is repeated until all the elements are sorted. It has best, average and worst case complexity of O(n log n). Visualization of Heap Sort . Implementation of Heap Sort . | Java | Python | C | . Copied! . Copied! . Copied! . ",
    "url": "https://gbhat.com/algorithms/heap_sort.html",
    "relUrl": "/algorithms/heap_sort.html"
  },"9": {
    "doc": "Insertion Sort",
    "weight": 1,
    "title": "Insertion Sort",
    "description": "Visualization and Implementation of Insertion Sort",
    "content": "Insertion Sort works similar to the way playing cards are sorted. It virtually partitions the array into sorted and unsorted set. At each iteration, it takes one element from unsorted partition and places it in the right location within sorted partition. This operation is repeated until no element is left in unsorted partition. It has best case complexity of O(n), average and worst case complexity of O($n^2$). Visualization of Insertion Sort . Implementation of Insertion Sort . | Java | Python | C | . Copied! . Copied! . Copied! . ",
    "url": "https://gbhat.com/algorithms/insertion_sort.html",
    "relUrl": "/algorithms/insertion_sort.html"
  },"10": {
    "doc": "Interpolation Search",
    "weight": 3,
    "title": "Interpolation Search",
    "description": "Visualization and Implementation of Interpolation Search",
    "content": "Interpolation Search is a searching algorithms which works on sorted array. It works best for arrays where elements are uniformly distributed. At every iteration it finds the position to search using $$pos = low + \\frac{(key - arr[low]) * (high - low)}{(arr[high] - arr[low])}$$ If element at the position is equal to the key, search stops. If the element at position is greater than the key, search continues in the first part. Otherwise search continues in the second part. It has best case complexity of O(1), average case complexity of O(log(log n)) and worst case complexity of O(n). Visualization of Interpolation Search . Implementation of Interpolation Search . | Java | Python | C | . Copied! . Copied! . Copied! . ",
    "url": "https://gbhat.com/algorithms/interpolation_search.html",
    "relUrl": "/algorithms/interpolation_search.html"
  },"11": {
    "doc": "Knights Tour Problem",
    "weight": 6,
    "title": "Knights Tour Problem",
    "description": "Visualization and Implementation of Knights Tour problem",
    "content": "The Knight’s tour is a puzzle in a N * N chessboard where Knight makes sequence of moves and must visit every square exactly once. This problem can be solved with backtracking. We begin the solution by placing knight at (0, 0). At every position, we try the possible moves to find a valid one. If we find a valid move, we move the knight to that position. If we don’t find such valid move, we backtrack and try to re-position the previous move. If we have completed all the squares of the board, we have a solution. If we have tried all possible moves and could not cover all the squares, we have failed to find a solution. Visualization of Knights Tour . Implementation of Knights Tour . | Java | Python | C | . Copied! . Copied! . Copied! . ",
    "url": "https://gbhat.com/algorithms/knights_tour.html",
    "relUrl": "/algorithms/knights_tour.html"
  },"12": {
    "doc": "Knuth-Morris-Pratt Algorithm",
    "weight": 4,
    "title": "Knuth-Morris-Pratt Algorithm",
    "description": "Visualization and Implementation of Knuth-Morris-Pratt String Search Algorithm",
    "content": "Knuth-Morris-Pratt String Search algorithm searches for a string (also called pattern) within larger string. It pre-computes a lookup table using the pattern. Lookup table helps in avoiding check for characters matches at each index of string. If characters do not match at any index, it uses the lookup table to shift the pattern where there is a possibility of match. It has worst case complexity of O(m + n). Where m is length of pattern and n is length of string. Visualization of Knuth-Morris-Pratt Algorithm . Implementation of Knuth-Morris-Pratt Algorithm . | Java | Python | C | . Copied! . Copied! . Copied! . ",
    "url": "https://gbhat.com/algorithms/kmp.html",
    "relUrl": "/algorithms/kmp.html"
  },"13": {
    "doc": "Linear Regression",
    "weight": 3,
    "title": "Linear Regression",
    "description": "Visualization and Implementation of Linear Regression",
    "content": "Figure 1: Iterative method to fit Linear Regression . Linear Regression is a supervised machine learning algorithm. Since this is a regression algorithm, it makes real valued (continuous valued) predictions. In constrast a classification algorithm makes descrete valued predictions. Linear Regression builds a linear relation (with constant slope) between feature values and target variables. Types of Linear Regressions . | In case of only one feature in Linear Regression, it is called Simple Linear Regression. Equation for this can be written as: $$ \\hat{y} = \\theta_1 * x_1 + \\theta_2 $$ . | Where $\\hat{y}$ is predicted value, $x_1$ is feature, $\\theta_1$, $\\theta_2$ are model parameters. | . | In case of more than one feature, it is called Multiple Linear Regression. Equation for this can be written as: $$ \\hat{y} = \\theta_1 * x_1 + \\theta_2 * x_2 + … + \\theta_n * x_n $$ . | In vector notation we can write it as: $ \\hat{y} = \\theta^T \\cdot X $ | $ \\theta = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\\\ .. \\\\ \\theta_n \\end{bmatrix} $ and $ X = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .. \\\\ x_n \\end{bmatrix} $ | $\\hat{y}$ is predicted value, $\\theta_1$, $\\theta_2$,…,$\\theta_n$ are model parameters, $x_1$, $x_2$,…, $x_n$ are feature values. | . | If more than one target is being predicted, it is called Multivariate Linear Regression. Equation for this can be written as: $$ \\hat{y}_1,\\hat{y}_2,…,\\hat{y}_n = \\theta_1 * x_1 + \\theta_2 * x_2 + … + \\theta_n * x_n $$ . | In vector notation we can write it as: $ Y = \\theta^T \\cdot X $ | $ \\theta = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\\\ .. \\\\ \\theta_n \\end{bmatrix} $ , $ Y = \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ .. \\\\ \\hat{y}_n \\end{bmatrix} $ and $ X = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .. \\\\ x_n \\end{bmatrix} $ | $\\hat{y}_1$, $\\hat{y}_2$,…, $\\hat{y}_n$ are predicted values, $\\theta_1$, $\\theta_2$,…,$\\theta_n$ are model parameters, $x_1$, $x_2$,…, $x_n$ are feature values. | . | . Approaches to fit Linear Regression Fitting the Linear Regression model is to find the model parameters ($\\theta$ vector). There are different approaches to fit Linear Regression. The most famous ones are: . | Normal Equation: Using normal equation, we can find all the values of model parameters directly and all at once. The equation is defined as: $$ \\theta = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot y $$ $ \\theta $ is model parameter vector, X is feature values vector and y is target values vector | Iterative method to reduce cost function approach reduces the cost function iteratively. Cost function is measure of how far are the predictions from actual target values. We use Mean Squared Error (MSE) as the cost function for Linear Regression. It is dfined as $$ MSE = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}_i - y_i)^2 $$ $ \\hat{y}_i $ is ith predicted value computed as $ \\hat{y} = \\theta^T \\cdot X $ $ y_i$ is ith target value, m is total number of data points. At each iteration, the $\\theta$ values are adjusted so that the MSE cost function reduced to minimum value. To minimize cost function, we can use Gradient Descent or similar optimization algorithms. Figure 1 shows how after each iteration, the Mean Squared Error value reduces and the prediction values reach close to target values. | . Implementation of Linear Regression . Copied! . ",
    "url": "https://gbhat.com/machine_learning/linear_regression.html",
    "relUrl": "/machine_learning/linear_regression.html"
  },"14": {
    "doc": "Linear Search",
    "weight": 1,
    "title": "Linear Search",
    "description": "Visualization and Implementation of Linear Search",
    "content": "Linear Search is a simple sequential searching algorithms. It works for any sorted or unsorted array. It sequentially checks each element in the array if it mathces with the given key. Search stops if any element matches with the key or if it reaches the end of the array. It has best case complexity of O(1), average case complexity of O($\\frac{n}{2}$) and worst case complexity of O(n). Visualization of Linear Search . Implementation of Linear Search . | Java | Python | C | . Copied! . Copied! . Copied! . ",
    "url": "https://gbhat.com/algorithms/linear_search.html",
    "relUrl": "/algorithms/linear_search.html"
  },"15": {
    "doc": "Merge Sort",
    "weight": 2,
    "title": "Merge Sort",
    "description": "Visualization and Implementation of Merge Sort",
    "content": "Merge Sort is am efficient comparison based sorting algorithm. It employs divide and conquer technique. It divides the array into n unsorted parts each containing one element. It merges the parts into sorted arrays until the complete array is rebuilt. It has best, average and worst case complexity of O(n log n). Visualization of Merge Sort . Implementation of Merge Sort . | Java | Python | C | . Copied! . Copied! . Copied! . ",
    "url": "https://gbhat.com/algorithms/merge_sort.html",
    "relUrl": "/algorithms/merge_sort.html"
  },"16": {
    "doc": "N Queens Problem",
    "weight": 5,
    "title": "N Queens Problem",
    "description": "Visualization and Implementation of N Queens problem",
    "content": "N Queens is the problem of placing N chess queens on a NxN chess board, so that no two queens can attack each other We can solve this problem using backtracking We begin from leftmost column and try to place the queen on a row such that previously placed queens don’t attack the newly placed queen If we find such block, we attempt to place the next queen on next column If we don’t find such a block in current column, we backtrack and attempt to re-place the queen on previous column If all the queens are placed on the board we have a solution If we run out of all the blocks on the board, we don’t have a solution . Visualization of N Queens . Implementation of N Queens . | Java | Python | C | . Copied! . Copied! . Copied! . ",
    "url": "https://gbhat.com/algorithms/n_queens.html",
    "relUrl": "/algorithms/n_queens.html"
  },"17": {
    "doc": "NGINX Uniform Load Balancing",
    "weight": 6,
    "title": "NGINX Uniform Load Balancing",
    "description": "Uniform load balancing using NGINX, experiments with docker",
    "content": "Figure 1: NGINX load balancer distributing the requests uniformly . A load balancer is used to distribute the incoming traffic to multiple web servers. We use load balancer when single web server is not able to handle all the traffic. NGINX provides load balancing ability along with other features. In this experiment we will see how to use NGINX for load balancing and run tests to see how it performs. Tools required to run the experiments: docker, docker-compose, httperf, curl(optional) . Web server without load balancer: . I have created a docker image with a simple HTTP server. This service replies with a fixed message and prints how many requests it received to the console. To mimic the server side work, this server adds a delay of 0.01s before replying. To run this web server we use the command: . sudo docker run --rm -p 8080:8080 -e \"SERVER_NAME=server1\" gbhat1/hello_http_server:latest . We can now use a web browser to reach http://localhost:8080 or we can use a command line tool like curl. To use curl we run the command: . curl -v http://localhost:8080 . Now let us run httperf to see the performance of this web server. The below command sends 1000 requests per second for 10 seconds. httperf --hog --server localhost --uri / --port 8080 --num-conn 10000 --num-call 1 --timeout 5 --rate 1000 . The output will depend on the computational power of the system. The below was one of the outputs. We can see that, server is able to handle ~666 connections/sec and an average of ~765 replies/sec. httperf --hog --timeout=5 --client=0/1 --server=localhost --port=8081 --uri=/ --rate=1000 --send-buffer=4096 --recv-buffer=16384 --num-conns=10000 --num-calls=1 httperf: warning: open file limit &gt; FD_SETSIZE; limiting max. # of open files to FD_SETSIZE Maximum connect burst length: 15 Total: connections 9661 requests 9661 replies 8168 test-duration 14.497 s Connection rate: 666.4 conn/s (1.5 ms/conn, &lt;=1022 concurrent connections) Connection time [ms]: min 2.3 avg 245.7 max 4732.6 median 8.5 stddev 813.6 Connection time [ms]: connect 0.0 Connection length [replies/conn]: 1.000 Request rate: 666.4 req/s (1.5 ms/req) Request size [B]: 62.0 Reply rate [replies/s]: min 764.4 avg 765.2 max 766.1 stddev 1.2 (2 samples) Reply time [ms]: response 244.4 transfer 1.3 Reply size [B]: header 116.0 content 8.0 footer 0.0 (total 124.0) Reply status: 1xx=0 2xx=8168 3xx=0 4xx=0 5xx=0 CPU time [s]: user 0.39 system 12.90 (user 2.7% system 89.0% total 91.6%) Net I/O: 108.6 KB/s (0.9*10^6 bps) Errors: total 1832 client-timo 1493 socket-timo 0 connrefused 0 connreset 0 Errors: fd-unavail 339 addrunavail 0 ftab-full 0 other 0 . Web servers with load balancer: . Now let us run the experiment with 3 web servers and a load balancer. I have created a docker image with NGINX configured as a load balancer. The docker image is using below configuration. We add the web server names and port details in upstream app_servers configuration. More details about the configuration can be found at: https://www.nginx.com/resources/wiki/start/topics/examples/full/ . worker_processes 4; events { worker_connections 2048; } http { access_log off; # List of application servers upstream app_servers { server server1:8080; server server2:8080; server server3:8080; } # Configuration for the server server { # Running port listen 8080; # Proxying the connections connections location / { proxy_pass http://app_servers; } } } . We use docker-compose to start NGINX and 3 web servers. We run the docker-compose command like this: . docker-compose -f nginx-docker-compose.yml up . nginx-docker-compose.yml: . version: '3' services: server1: image: gbhat1/hello_http_server:latest environment: \"SERVER_NAME\": \"server1\" server2: image: gbhat1/hello_http_server:latest environment: \"SERVER_NAME\": \"server2\" server3: image: gbhat1/hello_http_server:latest environment: \"SERVER_NAME\": \"server3\" nginx: image: gbhat1/nginx_lb_uniform:stable-alpine ports: - \"8080:8080\" links: - server1 - server2 - server3 depends_on: - server1 - server2 - server3 . If we use browser or curl to access http://localhost:8080 we can see each request is going to different web servers. We can send 10 requests using curl with below command: . seq 10 | xargs -Iz curl http://localhost:8080 . Let us run httperf to see the performance of this new setup: . httperf --hog --server localhost --uri / --port 8080 --num-conn 10000 --num-call 1 --timeout 5 --rate 1000 . This output may differ between systems. Below is one of the output which shows, server is able to handle all 1000 connections/sec and an average of ~999.9 replies/sec. httperf --hog --timeout=5 --client=0/1 --server=localhost --port=8080 --uri=/ --rate=1000 --send-buffer=4096 --recv-buffer=16384 --num-conns=10000 --num-calls=1 httperf: warning: open file limit &gt; FD_SETSIZE; limiting max. # of open files to FD_SETSIZE Maximum connect burst length: 7 Total: connections 10000 requests 10000 replies 10000 test-duration 10.000 s Connection rate: 1000.0 conn/s (1.0 ms/conn, &lt;=14 concurrent connections) Connection time [ms]: min 0.9 avg 2.2 max 24.6 median 1.5 stddev 1.3 Connection time [ms]: connect 0.0 Connection length [replies/conn]: 1.000 Request rate: 1000.0 req/s (1.0 ms/req) Request size [B]: 62.0 Reply rate [replies/s]: min 999.8 avg 999.9 max 1000.1 stddev 0.2 (2 samples) Reply time [ms]: response 2.2 transfer 0.0 Reply size [B]: header 155.0 content 8.0 footer 2.0 (total 165.0) Reply status: 1xx=0 2xx=10000 3xx=0 4xx=0 5xx=0 CPU time [s]: user 2.87 system 4.23 (user 28.7% system 42.3% total 71.0%) Net I/O: 219.7 KB/s (1.8*10^6 bps) Errors: total 0 client-timo 0 socket-timo 0 connrefused 0 connreset 0 Errors: fd-unavail 0 addrunavail 0 ftab-full 0 other 0 . These experiments show the way we can use NGINX as load balancer to increase the throughput of the system. ",
    "url": "https://gbhat.com/sysdesign/nginx_load_balancing.html",
    "relUrl": "/sysdesign/nginx_load_balancing.html"
  },"18": {
    "doc": "Optimization with Momentum",
    "weight": 4,
    "title": "Optimization with Momentum",
    "description": "Optimization with Momentum on Non-Convex function",
    "content": "Figure 1: Gradient Descent and RMSProp algorithms with and without momentum . | We saw an example of Gradient Descent with Optimization. Another algorithm which supports momentum optimization is RMSProp (Root Mean Square Propagation). | In this example we will use both the algorithms with optimization to find minimum of a non-convex function. | We can see that algorithms with momentum are able to find the global minima whereas without momentum are reaching only local minima. | We can also note that Gradient Descent with momentum is converging faster. | Below program shows usage of SGD and RMSProp with and without momentum. | . Implementation of Optimization with Momentum . Copied! . ",
    "url": "https://gbhat.com/machine_learning/optimize_with_momentum.html",
    "relUrl": "/machine_learning/optimize_with_momentum.html"
  },"19": {
    "doc": "Particle Swarm Optimization on Convex Function",
    "weight": 5,
    "title": "Particle Swarm Optimization on Convex Function",
    "description": "Particle Swarm Optimization on Convex Function",
    "content": "Figure 1: Particle swarm optimization on a convex function . We saw different gradient based optimizations in the previous examples. Particle swarm optimization is a Population Method. The algorithms starts by initializing multiple particles in the search-space. Each particle keeps track of its local best known position and also the best global position. Particle’s position and velocity are updated iteratively, such that the position moves towards the best solution. Since multiple particles are searching for the best solution in the search-space, there is a higher possibility of finding the global best solution. Steps of the algorithm are as below: . Initialize particles position (vector $ x_i $), each particles’ best known position (vector $ p_i $), each particle’s initial velocity (vector $ v_i $) . At each iteration update the velocity and position: $$ x_i = x_i + v_i $$ $$ v_i = w * v_i + c_1 * r_1 * (p_i - x_i) + c_2 * r_2 * (p_{best} - x_i) $$ . Where . $ w $ is inertia weight . $ c_1 $ and $ c_2 $ are momentum coefficients or social coefficients . $ r_1 $ and $ r_2 $ are random numbers in the range U(0, 1) . $ p_i $ is the best position found for the current particle . $ p_{best} $ is the best position across all the particles . Below example uses pyswarms package to optimize a convex function. We can see better application of particle swarm approach for non-convex functions in Particle Swarm Optimization on Non-Convex Function and Particle Swarm Optimization on Six-Hump Camel Back Function . Implementation of Particle Swarm Optimization on Convex Function . Copied! . ",
    "url": "https://gbhat.com/machine_learning/pso_convex.html",
    "relUrl": "/machine_learning/pso_convex.html"
  },"20": {
    "doc": "Particle Swarm Optimization on Non-Convex Function",
    "weight": 5,
    "title": "Particle Swarm Optimization on Non-Convex Function",
    "description": "Particle Swarm Optimization on Non-Convex Function",
    "content": "In the previous example we saw the details of Particle Swarm Optimization (PSO) and how it performs on a convex function. Particle swarm can be used to efficiently find the global minima of non-convex functions. In this example, we will use a non-convex function which has local minima and global minima and see how particle swarm performs on it. This non-convex function is defined as: $$ (1 - x/2 + x^5 + y^3) * e^{(-x^2 - y^2)} $$ Where $ x, y \\in (-3, 3) $ . This function has a local minima at (0.11, -1.46, -0.25) and a global minima at (-1.69, 0, -0.69) . We can see the visualization of this function below: . Figure 1: Non-convex function having a global and a local minima . Below animation shows how PSO performs on this function: . Figure 2: Particle swarm optimization on non-convex function . Below program uses pyswarms library to find the minima of this non-convex function. To install this library we can follow the steps at Installation . Implementation of Particle Swarm Optimization on Non-Convex Function . Copied! . ",
    "url": "https://gbhat.com/machine_learning/pso_nonconvex.html",
    "relUrl": "/machine_learning/pso_nonconvex.html"
  },"21": {
    "doc": "Particle Swarm Optimization on Non-Convex Function",
    "weight": 6,
    "title": "Particle Swarm Optimization on Non-Convex Function",
    "description": "Particle Swarm Optimization on Non-Convex Six-Hump Camel Back Function",
    "content": "We saw how particle swarm optimization(PSO) works on convex function and a non-convex function In this example, we will see how PSO performs on another non-convex function called Six-Hump Camel Back Function . This function is defined as: $$ (4 - 2.1 * x^2 + x^4 / 3) * x^2 + x * y + (-4 + 4 * y^2) * y^2 $$ . Where $ x \\in (-3, 3), y \\in (-2, 2) $ . The function has six local minima, two of which are global. Two equi-valued global minima are at (0.09, -0.71, -1.03) and (-0.09, 0.71, -1.03). We can see that the global minima value is -1.03 for x and y values of (0.09, -0.71) and (-0.09, 0.71). Visualization of Six-Hump Camel Back Function is below: . Figure 1: Six-hump camel back function . If we run PSO on this function, it will converge at either of the two global minima. We can see PSO in action in the below animation: . Figure 2: Particle swarm optimization on six-hump camelback function . Below program uses pyswarms library to find the minima of this non-convex function. To install this library we can follow the steps at Installation . Implementation of Particle Swarm Optimization on Six-Hump Camel Back Function . Copied! . ",
    "url": "https://gbhat.com/machine_learning/pso_shcb.html",
    "relUrl": "/machine_learning/pso_shcb.html"
  },"22": {
    "doc": "Polynomial Regression",
    "weight": 3,
    "title": "Polynomial Regression",
    "description": "Visualization and Implementation of Polynomial Regression",
    "content": "Figure 1: Iterative method to fit Polynomial Regression . Linear Regression builds a linear relation between features and target variable. But if features and target variables are not linearly related, we can try to fit a polynomial relation between them. Mathematically polynomial regression is of the form: . $$ \\hat{y} = \\theta_0 + \\theta_1 * x + \\theta_2 * x^2 + … + \\theta_n * x^n $$ . This equation can be viewed as a Multiple Linear Regression: . $$ \\hat{y} = \\theta_0 + \\theta_1 * x_1 + \\theta_2 * x_2 + … + \\theta_n * x_n $$ Where $x_1 = x$, $x_2 = x^2$, $x_n = x^n$ . Frameworks provide utility functions to generate polynomial features from input features. scikit-learn provides PolynomialFeatures(degree=…) for this purpose. Once input is converted to polynomial features, we can use the approaches used for Linear Regression to fit the model. In the below example we will generate non-linear data and use scikit-learn to fit the model. Implementation of Polynomial Regression . Copied! . ",
    "url": "https://gbhat.com/machine_learning/polynomial_regression.html",
    "relUrl": "/machine_learning/polynomial_regression.html"
  },"23": {
    "doc": "Polynomial Regression 2",
    "weight": 3,
    "title": "Polynomial Regression 2",
    "description": "Polynomial Regression on Linköping temperature data",
    "content": "Figure 1: Fitting a polynomial regression model to Linköping temperature data . We used a simulated data to fit a polynomial model in Polynomial Regression. In this example we will use a real world dataset and fit a polynomial model. Below program uses dataset file which can be downloaded from: TempLinkoping2016.csv . Implementation of Polynomial Regression to fit Linköping temperature data . Copied! . ",
    "url": "https://gbhat.com/machine_learning/polynomial_regression_2.html",
    "relUrl": "/machine_learning/polynomial_regression_2.html"
  },"24": {
    "doc": "Polynomial Regression Under Fit, Good Fit, Over Fit",
    "weight": 3,
    "title": "Polynomial Regression Under Fit, Good Fit, Over Fit",
    "description": "Polynomial Regression Under Fit, Good Fit, Over Fit",
    "content": "Figure 1: Polynomial regression under fit, good fit and over fit . If we are fitting a polynomial regression model to our data, we need to decide on the degree of the polynomial. Using an appropriate degree is very important since using a lower degree may underfit the data and usnig a higher degree may overfit. In the below example we will fit polynomials of different degrees and see underfit, good fit and overfit models. This program uses dataset file which can be downloaded from: TempLinkoping2016.csv . Implementation of Polynomial Regression Under Fit, Good Fit, Over Fit . Copied! . ",
    "url": "https://gbhat.com/machine_learning/polynomial_regression_fit.html",
    "relUrl": "/machine_learning/polynomial_regression_fit.html"
  },"25": {
    "doc": "Quick Sort",
    "weight": 2,
    "title": "Quick Sort",
    "description": "Visualization and Implementation of Quick Sort",
    "content": "Quick sort is a very efficient sorting algorithm and most commonly used for sorting. It uses divide and conquer technique. It chooses one element as pivot (We are using last element as pivot). It partitions the other elements into two parts, where elements in first part are less than the pivot and elements in second part are higher than the pivot. Each partition is sorted recursively. It has best and average case complexity of O(n log n) and worst case complexity of O($n^2$). Visualization of Quick Sort . Implementation of Quick Sort . | Java | Python | C | . Copied! . Copied! . Copied! . ",
    "url": "https://gbhat.com/algorithms/quick_sort.html",
    "relUrl": "/algorithms/quick_sort.html"
  },"26": {
    "doc": "Scikit-Learn ML pipeline for regression",
    "weight": 6,
    "title": "Scikit-Learn ML pipeline for regression",
    "description": "Scikit-Learn pipeline for fuel efficiency regression",
    "content": "Figure 1: Scikit-Learn ML pipeline with multiple transformations . In this post we will see how we can use Scikit-Learn pipelines to transform the data and train ML models. Scikit-Learn pipelines can have multiple stages which will be run in specific order. These stages can be either Transformers or Estimators. We can see more details on Scikit-Learn ML Pipelines here. Auto MPG dataset . We will take the example dataset of measuring fuel efficiency of cars. The original dataset is available here I have taken this data and pre-processed it a bit to use it easily in this example (adding headers, change column delimiter etc.) This dataset which we will use can be downloaded from here . This data contains features: MPG, Cylinders, Displacement, Horsepower, Weight, Acceleration, Model Year, Origin, Car Name. MPG specifies the fuel efficiency of the car. Reading the data . To read the data, we will use Pandas library like below: . mpg_data = pd.read_csv('auto-mpg.csv.gz', sep='|') . We can see the details of the loaded data like type of column, count of each field using: . mpg_data.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 398 entries, 0 to 397 Data columns (total 9 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 MPG 398 non-null float64 1 Cylinders 398 non-null int64 2 Displacement 398 non-null float64 3 Horsepower 392 non-null float64 4 Weight 398 non-null float64 5 Acceleration 398 non-null float64 6 Model Year 398 non-null int64 7 Origin 398 non-null object 8 Car Name 398 non-null object dtypes: float64(5), int64(2), object(2) memory usage: 28.1+ KB . We can see that columns except Origin and Car Name are of numeric type. Horsepower is missing 6 values. We will fill these values using Imputer. Origin is a non-numeric column. Let us see the distinct values in the column: . mpg_data['Origin'].unique() . array(['USA', 'Japan', 'Europe'], dtype=object) . Origin is a categorical column. We will convert it to one-hot encoding. Simlilarly Car Name is a String column. However it does not having any relation with fuel efficiency. We will drop this column. mpg_data = mpg_data.drop('Car Name', axis=1) . Visualizing the data . Let us visualize this data using Seaborn. A pairplot renders pairwise relationship in the dataset. Pairplot of different columns can be seen using: . sns.pairplot(mpg_data[['MPG', 'Cylinders', 'Displacement', 'Weight', 'Horsepower']], diag_kind='kde') . Figure 1: Spark ML pipeline with multiple transformations . To see pairwise correlation of the features we use the heatmap: . corr = mpg_data.corr() sns.heatmap(corr, annot=True) . Figure 2: Correlation heatmap . We can see in the scatter plot and correlation matrix that as the horsepower, weight and displacement increase, MPG is reducing. Build pipeline and transform the data . We will transform only features not the labels. So we split the features and labels in the data: . mpg_data_labels = mpg_data['MPG'].copy().to_numpy() mpg_data_features = mpg_data.drop('MPG', axis=1) . We will build two pipelines to transform the data. One of the pipeline is for numeric columns which will use these Transformers: Imputer: Since we have missing values in Horsepower column, we will fill the missing data using median of all the remaining values. StandardScaler: The features in the dataset are not in the same range. Machine learning algorithms perform well when data is in similar ranges. We will use StandarScaler to scale the input data. This pipeline is defined as below: . num_tr_pipeline = Pipeline([ ('imputer', SimpleImputer(strategy=\"median\")), ('std_scaler', StandardScaler()), ]) . Second pipeline is to transform categorical feature ‘Origin’ to one-hot encoding. This pipeline will have below Transformers: OrdinalEncoder: Before we convert Origin to a one hot encoding we need to assign label indicies to each unique value. OrdinalEncoder will perform this operation. OneHotEncoder: Once OrdinalEncoder stage is completed, we can convert the categorical values to one-hot encoding using this transformation. This pipeline is defined as below: . cat_tr_pipeline = Pipeline([ ('ordinal_encoder', OrdinalEncoder()), ('one_hot_encoder', OneHotEncoder()), ]) . We need to specify which features should be processed by each pipeline. Since features except Origin are numeric, they should be processed by numeric pipeline. We define the numeric columns using: . num_attribs = [col for col in mpg_data_features.columns if col != 'Origin'] . Origin column should be processed by categorical pipeline. We define the categorical column: . cat_attribs = ['Origin'] . Now we can transform the columns using the pipelines: . full_pipeline = ColumnTransformer([ (\"num_tr_pipeline\", num_tr_pipeline, num_attribs), (\"cat_tr_pipeline\", cat_tr_pipeline, cat_attribs), ]) mpg_transformed = full_pipeline.fit_transform(mpg_data_features) . mpg_transformed will have all the data transformations applied. Build the model, predict and evaluate the model . Before we build the model, we split the data into train and test data: . mpg_train_data, mpg_test_data, mpg_train_labels, mpg_test_labels = train_test_split(mpg_transformed, mpg_data_labels, test_size=0.3, random_state=0) . First let us try a simple Linear Regression model. Train the model using train data and evaluate how it performs on the test data: . lin_reg = LinearRegression() lin_reg.fit(mpg_train_data, mpg_train_labels) mpg_test_predicted = lin_reg.predict(mpg_test_data) np.sqrt(mean_squared_error(mpg_test_labels, mpg_test_predicted, squared=True)) . This gives output of: . 3.366034587971452 . We will build another model using RandomForestRegressor: . from sklearn.ensemble import RandomForestRegressor forest_reg = RandomForestRegressor(n_estimators=50, random_state=0) forest_reg.fit(mpg_train_data, mpg_train_labels) mpg_test_predicted = forest_reg.predict(mpg_test_data) np.sqrt(mean_squared_error(mpg_test_predicted, mpg_test_labels, squared=True)) . This gives output of: . 2.7287667360915995 . This is improvement over Linear Regression. We can further tweak the model parameters or build different models to further improve the prediction. Since the goal of this post was to show the usage of Scikit-Learn ML pipelines, we will stop here. Complete implementation of Scikit-Learn ML Pipeline for regression . This is available as Jupyter Notebook and as a Python script . ",
    "url": "https://gbhat.com/machine-learning/scikit_learn_ml_regression_pipeline.html",
    "relUrl": "/machine-learning/scikit_learn_ml_regression_pipeline.html"
  },"27": {
    "doc": "Selection Sort",
    "weight": 1,
    "title": "Selection Sort",
    "description": "Visualization and Implementation of Selection Sort",
    "content": "Selection Sort is an in-place, comparison based sorting. It virtually partitions the array into sorted and unsorted set. At each iteration, it takes the minimum element from unsorted partition and places it in the right location within sorted partition. This operation is repeated until no element is left in unsorted partition. It has best, average and worst case complexity of O($n^2$). Thus not very well suited for large arrays. Visualization of Selection Sort . Implementation of Selection Sort . | Java | Python | C | . Copied! . Copied! . Copied! . ",
    "url": "https://gbhat.com/algorithms/selection_sort.html",
    "relUrl": "/algorithms/selection_sort.html"
  },"28": {
    "doc": "Gradient Descent vs L-BFGS-B",
    "weight": 6,
    "title": "Gradient Descent vs L-BFGS-B",
    "description": "Performance of Stochastic Gradient Descent vs L-BFGS-B on functions",
    "content": "Gradient Descent: We saw the basic details of Gradient Descent in the previous example. Gradient descent is defined as first-order iterative optimization algorithm to find minima of a differentiable function. Mathematical definition of gradient descent is: $$ x_{n+1} = x_n - \\alpha * \\frac {d f(x)}{dx_n} $$ . $\\alpha$ is learning rate. The value of $x_{n+1}$ is computed iteratively until it converges at a minimum. It is called as a first-order because we use the first order differentiation of the function. Newton’s Methods use the second order differentiation of function to converge at the minima faster. To decide the next point on the gradient, Newtons methods use the Hessian matrix. Hessian matrix for a function $ f(x) $ is defined as: . $$H_f = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; … &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n}\\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; … &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n}\\\\ . &amp; . &amp; &amp; . \\\\ . &amp; . &amp; &amp; . \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; … &amp; \\frac{\\partial^2 f}{\\partial x_n^2}\\\\ \\end{bmatrix}$$ . or by using the indices i and j: . $$(H_f)_{i,j} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$$ . Newton’s method performs the below iteration to compute the next point on the gradient: $$ x_{n+1} = x_n - \\alpha H^{-1} \\frac {d f(x)}{dx_n} $$ . Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm is a quasi-Newton method. Quasi-Newton methods are alternative to Newton’s method. The expensive operation in Newton’s method is computation of inverse of Hessian matrix ($H^{-1}$). Instead Quasi-Newton methods compute a matrix which is an approximation to Hessian matrix. However BFGS still uses a $n \\times n$ memory to store the approximation matrix. If the machine learning model has large number of parameters (hundreds of thousands to millions), this will be very large to fit in memory. Limited Memory BFGS (L-BFGS) is an improvement over BFGS to optimize memory usage. L-BFGS uses the approximation to Hessian matrix approach. But instead of storing a large $n \\times n$ matrix as approximation to Hessian, it stores a set of vectors. This reduces memory usage significantly and it can used as optimization algorithm to build large machine learning models. L-BFGS-B extends L-BFGS to additionally provide support for boundary constraints on variables. We will use the L-BFGS-B algorithm provided in scipy library to minimize functions and compare it with gradient descent from Tensorflow library. Below animations show the difference between gradient descent and L-BFGS to reach to the minimum for two functions: . Figure 1: Gradient Descent vs L-BFGS-B on function $ y = x^2 $ where $ x \\epsilon (-10, 10) $ . Figure 2: Gradient Descent vs L-BFGS-B on function $ y = e ^ {-x/2} $ where $ x \\epsilon (-10, 0) $ . References: . | Deep Learning Book Section 8.6.1 covers Stochastic Gradient Descent and Section 8.6.3 covers BFGS, L-BFGS . | Wikiepdia articles on LBFGS, BFGS, Newton’s Methods, Quasi-Newton Methods, Hessian Matrix . | . Implementation of Gradient Descent and L-BFGS-B for $y = x^2$ . Copied! . Implementation of Gradient Descent and L-BFGS-B for $y = e ^ {-x/2}$ . Copied! . ",
    "url": "https://gbhat.com/machine_learning/sgd_vs_lbfgsb.html",
    "relUrl": "/machine_learning/sgd_vs_lbfgsb.html"
  },"29": {
    "doc": "Spark ML pipeline for regression",
    "weight": 6,
    "title": "Spark ML pipeline for regression",
    "description": "Spark ML pipeline for fuel efficiency regression",
    "content": "Figure 1: Spark ML pipeline with multiple transformations . This post shows the usage of Spark ML pipelines to transform the data and train the models. Spark ML pipelines consist of multiple stages which will be run in specific order. These stages can be either Transformers or Estimators. We can see more details on Spark ML Pipelines here . Auto MPG dataset . We will take the example dataset of measuring fuel efficiency of cars. The original dataset is available here I have taken this data and pre-processed it a bit to use it easily in this example (adding headers, change column delimiter etc.) This dataset which we will use can be downloaded from here . This data contains features: MPG, Cylinders, Displacement, Horsepower, Weight, Acceleration, Model Year, Origin, Car Name. MPG specifies the fuel efficiency of the car. Visualizing the data . We can load this data and visualize it either using Python or Tableau. The Tableau view for scatter plots is shown below: . Figure 1: Spark ML pipeline with multiple transformations . The correlation matrix for the features is below: . +------------+------+---------+------------+----------+------+------------+ |MPG |Cylinders|Displacement|Horsepower|Weight|Acceleration| +------------+------+---------+------------+----------+------+------------+ |MPG |+1.000|-0.778 |-0.805 |-0.778 |-0.832|+0.423 |Cylinders |-0.778|+1.000 |+0.951 |+0.843 |+0.898|-0.505 |Displacement|-0.805|+0.951 |+1.000 |+0.897 |+0.933|-0.544 |Horsepower |-0.778|+0.843 |+0.897 |+1.000 |+0.865|-0.689 |Weight |-0.832|+0.898 |+0.933 |+0.865 |+1.000|-0.417 |Acceleration|+0.423|-0.505 |-0.544 |-0.689 |-0.417|+1.000 | +------------+------+---------+------------+----------+------+------------+ . We can see in the scatter plot and correlation matrix that as the horsepower, weight and displacement increase, MPG is reducing. Reading and transforming the data . Now let us read the data and see the columns, type of columns and more details. To read data, we will use: . spark.read() .option(\"header\", true) .option(\"inferSchema\", true) .option(\"delimiter\", \"|\") .csv(\"auto-mpg.csv.gz\"); . We are using these options to read the data: . | • header specifies that there is a header line in the csv file | • inferSchema to automatically determine the type of data in each column | • delimiter to sepcify that we are using pipe(‘|’) as the delimiter between the fields | . If we print the schema to see the type of each column: . inDs.printSchema(); . This shows the output: . root &#124;-- MPG: double (nullable = true) &#124;-- Cylinders: integer (nullable = true) &#124;-- Displacement: double (nullable = true) &#124;-- Horsepower: double (nullable = true) &#124;-- Weight: double (nullable = true) &#124;-- Acceleration: double (nullable = true) &#124;-- Model Year: integer (nullable = true) &#124;-- Origin: string (nullable = true) &#124;-- Car Name: string (nullable = true) . Schema shows that the columns except Origin and Car Name are of numeric type. Let us see the unique values in the Origin column: . inDs.select(\"Origin\").distinct().show(); . +------+ |Origin| +------+ |Europe| USA| Japan| +------+ . Origin is a categoric value, we will convert it to one hot encoding. Similarly car name is also a string feature. Since car name is not having any relation to the fuel efficiency, we will drop this column. inDs = inDs.drop(\"Car Name\"); . Let us see the summary of all the data including the count of values in each column using: . inDs.summary().show(); . It shows the below output: . +-------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------+ |summary| MPG| Cylinders| Displacement| Horsepower| Weight| Acceleration| Model Year|Origin| +-------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------+ | count| 398| 398| 398| 392| 398| 398| 398| 398| mean|23.514572864321615| 5.454773869346734|193.42587939698493|104.46938775510205|2970.424623115578|15.568090452261291| 76.01005025125629| null| stddev| 7.815984312565783|1.7010042445332123|104.26983817119587| 38.49115993282846|846.8417741973268| 2.757688929812676|3.6976266467325862| null| min| 9.0| 3| 68.0| 46.0| 1613.0| 8.0| 70|Europe| 25%| 17.5| 4| 104.0| 75.0| 2223.0| 13.8| 73| null| 50%| 23.0| 4| 146.0| 93.0| 2800.0| 15.5| 76| null| 75%| 29.0| 8| 262.0| 125.0| 3609.0| 17.2| 79| null| max| 46.6| 8| 455.0| 230.0| 5140.0| 24.8| 82| USA| +-------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------+ . We can see that Horsepower has missing values. We will have to fill in those missing values. Also we can note that each feature has different ranges. We can build the pipeline to transform the data to the required format before we train the models. Transformer and the purpose of it is summarized below: . Imputer: Since we have missing values in Horsepower column, we will fill the missing data using median of all the remaining values. StringIndexer: Origin is a categorical column. Before we convert it to a one hot encoding we need to assign label indicies to each unique value. StringIndexer will perform this operation. We can see the assigned values below: . +------+---------+ |Origin|OriginIdx| +------+---------+ | Japan| 1.0|Europe| 2.0| USA| 0.0| +------+---------+ . OneHotEncoder: Once StringIndexer stage is completed, we can convert the categorical values to one-hot encoding using this transformation. Once this stage completes the transformed categorical value looks like this: . +------+-------------+ &#124;Origin&#124; OriginVec&#124; +------+-------------+ &#124; USA&#124;(2,[0],[1.0])&#124; &#124; Japan&#124;(2,[1],[1.0])&#124; &#124;Europe&#124; (2,[],[])&#124; +------+-------------+ . OriginVec is a sparse vector which shows the nonzero indices and their values. Sparse Vector is an efficient format for this data since at most only one value is set in the vector. But if we want to see this vector in dense format, we can convert this sparse vector to dense: . transformedDs.select(\"Origin\", \"OriginVec\") .withColumn(\"OriginVecDense\", functions.udf((UDF1&lt;Vector, Object&gt;) Vector::toDense, new VectorUDT()) .apply(transformedDs.col(\"OriginVec\"))).distinct().show(); . One hot encoded Origin column in sparse and dense format looks like: . +------+-------------+--------------+ &#124;Origin&#124; OriginVec&#124;OriginVecDense&#124; +------+-------------+--------------+ &#124; Japan&#124;(2,[1],[1.0])&#124; [0.0,1.0]&#124; &#124; USA&#124;(2,[0],[1.0])&#124; [1.0,0.0]&#124; &#124;Europe&#124; (2,[],[])&#124; [0.0,0.0]&#124; +------+-------------+--------------+ . VectorAssembler: Vector assembler combines multiple columns into one vector format. We will combine all the transformed columns into one vector. StandardScaler: The features in the dataset are not in the same range. Machine learning algorithms perform well when data is in similar ranges. We will use StandarScaler to scale the data. Build the model, predict and evaluate the model . Once the data is transformed and is ready for training, we can build the model. At first let us split the data and set aside a test data set . Dataset&lt;Row&gt;[] splits = transformedDs.randomSplit(new double[] {0.7, 0.3}, 0); Dataset&lt;Row&gt; trainDs = splits[0]; Dataset&lt;Row&gt; testDs = splits[1]; . First model we will try is a LinearRegression. We can build the model by using: . LinearRegression lr = new LinearRegression() .setRegParam(0.3) .setFeaturesCol(\"TrainFeaturesScaled\") .setLabelCol(\"MPG\"); LinearRegressionModel lrModel = lr.fit(trainDs); . We are setting the train features and the label columns. We are also setting a regularization parameter which will help in better generalization Once the model is built, we can predict the values of the test data. By default the predicted values are in the column “prediction”. Let us see the 10 predicted values and the expected value. Dataset&lt;Row&gt; predictions = lrModel.transform(testDs); predictions.selectExpr(\"MPG\", \"round(prediction, 3) as prediction\").show(10); . +----+----------+ | MPG|prediction| +----+----------+ | 9.0| 7.643|11.0| 15.812|12.0| 11.554|13.0| 16.405|13.0| 11.631|13.0| 12.56|13.0| 9.444|13.0| 7.372|14.0| 14.028|14.0| 13.481| +----+----------+ . We can see some of the predictions are close to the actual values. We can quantitatively measure the models performance. RMSE (Root Mean Squared Error) is a performance measure for regression models. We can measure the RMSE of this model by: . RegressionEvaluator evaluator = new RegressionEvaluator() .setLabelCol(\"MPG\") .setPredictionCol(\"prediction\") .setMetricName(\"rmse\"); . This gives us the output: . Linear Regression RMSE on test data = 3.606182319282716 . Let us build another model using RandomForestRegressor. This can be done by using: . RandomForestRegressor rfRegressor = new RandomForestRegressor() .setNumTrees(8) .setSeed(0) .setFeaturesCol(\"TrainFeaturesScaled\") .setLabelCol(\"MPG\"); RandomForestRegressionModel rfModel = rfRegressor.fit(trainDs); . If we measure the RMSE of this model using the same approach as above, we can see that: . Random Forest Regression RMSE on test data = 2.9796895291729486 . This is better performance compared to Linear Regression model. We can further tweak the model parameters or build different models to further improve the prediction. Since the goal of this post was to show the usage of Spark ML pipelines and how we can use them to train regression models, we will stop here. Complete implementation of Spark ML Pipeline for regression . Copied! . ",
    "url": "https://gbhat.com/spark/spark_ml_regression_pipeline.html",
    "relUrl": "/spark/spark_ml_regression_pipeline.html"
  },"30": {
    "doc": "Spark Pushdown Optimizations",
    "weight": 6,
    "title": "Spark Pushdown Optimizations",
    "description": "Spark projection, predicate, aggregate, group-by pushdown optimizations",
    "content": "Figure 1: Spark queries with pushdown optimization . Spark supports loading data from different sources (databases, CSV/parquet/ORC files etc). This involves copying the data from data sources into Spark typically over the network. To optimize this data transfer, Spark has pushdown optimizations which reduce the amount of data to be transferred. We can see different pushdown optimizations below. Projection pushdown: . If we are fetching many columns of the data, and later if we use only fewer number of columns, Spark will fetch only the used columns. This feature is also known as projection pruning. We can see in the below example, we fetched all the columns from table city, we selected only name and population to be used later on. Spark optimizes this to fetch only the name and population columns. Dataset&lt;Row&gt; ds1 = session.sql(\"select * from h2.citydb.city\"); ds1 = ds1.select(\"name\", \"population\"); ds1.explain(); . We can see in the optimized physical plan that ReadSchema has only name and population columns: . *(1) Scan org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCScan$$anon$1@20a46227 [NAME#25,POPULATION#27L] PushedAggregates: [], PushedFilters: [], PushedGroupby: [], ReadSchema: struct&lt;NAME:string,POPULATION:bigint&gt; Predicate pushdown: . Projection pushdown reduces the number of columns to be fetched. Predicate pushdown reduces the number of rows to be fetched from the underlying storage. Predicate is a condition which is in the where/filter conditions. If we are reducing the number of records by using these conditions, Spark will pushdown this operation to underlying data storage if the data storage supports the operation. In this example we are filtering the city records by using condition population &gt;= 5000000. Dataset&lt;Row&gt; ds2 = session.sql(\"select * from h2.citydb.city where population &gt;= 5000000\"); ds2.explain(true); . This condition can be seen in PushedFilters in the optimized physical plan: . *(1) Scan org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCScan$$anon$1@5315210d [NAME#46,COUNTRY#47,POPULATION#48L] PushedAggregates: [], PushedFilters: [IsNotNull(POPULATION), GreaterThanOrEqual(POPULATION,5000000)], PushedGroupby: [], ReadSchema: struct&lt;NAME:string,COUNTRY:string,POPULATION:bigint&gt; Aggregate and group by pushdown: . Spark will push down operations like group by and aggregations like MIN, MAX, SUM to the underlying storage whenever possible. In the below example, we have a where condition, a group by operation and aggregation operations: . Dataset&lt;Row&gt; ds3 = session.sql(\"select country, MAX(population), \" + \"MIN(population) from h2.citydb.city where country in ('China', 'India') group by country\"); ds3.explain(true); . The optimized physical plan shows the pushed operations: . Scan org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCScan$$anon$1@779cb140 [COUNTRY#72,MAX(POPULATION)#82L,MIN(POPULATION)#83L] PushedAggregates: [MAX(POPULATION), MIN(POPULATION)], PushedFilters: [In(COUNTRY, [China,India])], PushedGroupby: [COUNTRY], ReadSchema: struct&lt;COUNTRY:string,MAX(POPULATION):bigint,MIN(POPULATION):bigint&gt; Complete implementation of Spark Pushdown Optimizations Example . Copied! . ",
    "url": "https://gbhat.com/spark/spark_pushdown_optimizations.html",
    "relUrl": "/spark/spark_pushdown_optimizations.html"
  },"31": {
    "doc": "Tower Of Hanoi",
    "weight": 3,
    "title": "Tower Of Hanoi",
    "description": "Visualization and Implementation of Tower Of Hanoi",
    "content": "Tower of Hanoi is a mathematical puzzle. It consists of three rods and number of disks of different sizes. In the beginning all the disks are placed in the source rod, in sorted order with smallest on top. The goal is to move all the disks from source rod to target rod using an auxiliary rod by obeying these rules: . | • Only one disk can be moved at a time. | • Each move can take the disk on top of any rod and move to another rod. | • Larger disk should not be placed on a smaller disk at any time. | . Visualization of Tower Of Hanoi . Implementation of Tower Of Hanoi . | Java | Python | C | . Copied! . Copied! . Copied! . ",
    "url": "https://gbhat.com/algorithms/tower_of_hanoi.html",
    "relUrl": "/algorithms/tower_of_hanoi.html"
  }
}
